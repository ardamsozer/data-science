{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W205: Project 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As directed, we use the following command to download the data:\n",
    "```\n",
    "    curl -L -o assessment-attempts-20180128-121051-nested.json https://goo.gl/ME6hjp`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1a) Command to bring up the cluster:\n",
    "```\n",
    "    docker-compose up -d\n",
    "```\n",
    "### 1b) Optionally, check that the cluster is up by running the following 2 commands:\n",
    "```\n",
    "    docker-compose ps\n",
    "    docker-compose ps -a\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a) command to create the kafka topic with a the name \"assessments\":\n",
    "```\n",
    "docker-compose exec kafka kafka-topics --create --topic assessments --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
    "```\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "### 2b) We can describe the topic using the command,\n",
    "```\n",
    "docker-compose exec kafka kafka-topics --describe --topic assessments --zookeeper zookeeper:32181\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) command to publish the assessments json data to the kafka topic using kafkacat\n",
    "```\n",
    "docker-compose exec mids bash -c \"cat /w205/project-2-asozer/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t assessments\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) command to shutdown the cluster:\n",
    "```\n",
    "docker-compose down\n",
    "```\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "### 4b) We can check that the cluster is teared down using:\n",
    "\n",
    "```\n",
    "docker-compose ps\n",
    "```\n",
    "and \n",
    "\n",
    "```\n",
    "docker ps -a \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) create a data frame by subscribing to the kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_assessments = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"kafka:29092\").option(\"subscribe\",\"assessments\").option(\"startingOffsets\", \"earliest\").option(\"endingOffsets\", \"latest\").load() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "| key|               value|      topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     0|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     1|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     2|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     3|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     4|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     5|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     6|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     7|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     8|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     9|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    10|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    11|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    12|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    13|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    14|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    15|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    16|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    17|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    18|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|    19|1969-12-31 23:59:...|            0|\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_assessments.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) convert the json data as a string to a new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_assessments.cache()\n",
    "\n",
    "assessments = raw_assessments.select(raw_assessments.value.cast('string'))\n",
    "assessments.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) extract / unroll the json data into new dataframes to answer you business questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
      "|        base_exam_id|certification|           exam_name|   keen_created_at|             keen_id|    keen_timestamp|max_attempts|           sequences|          started_at|        user_exam_id|\n",
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717442.735266|5a6745820eb8ab000...| 1516717442.735266|         1.0|Map(questions -> ...|2018-01-23T14:23:...|6d4089e4-bde5-4a2...|\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717377.639827|5a674541ab6b0a000...| 1516717377.639827|         1.0|Map(questions -> ...|2018-01-23T14:21:...|2fec1534-b41f-441...|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...| 1516738973.653394|5a67999d3ed3e3000...| 1516738973.653394|         1.0|Map(questions -> ...|2018-01-23T20:22:...|8edbc8a8-4d26-429...|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...|1516738921.1137421|5a6799694fc7c7000...|1516738921.1137421|         1.0|Map(questions -> ...|2018-01-23T20:21:...|c0ee680e-8892-4e6...|\n",
      "|6442707e-7488-11e...|        false|Introduction to B...| 1516737000.212122|5a6791e824fccd000...| 1516737000.212122|         1.0|Map(questions -> ...|2018-01-23T19:48:...|e4525b79-7904-405...|\n",
      "|8b4488de-43a5-4ff...|        false|        Learning Git| 1516740790.309757|5a67a0b6852c2a000...| 1516740790.309757|         1.0|Map(questions -> ...|2018-01-23T20:51:...|3186dafa-7acf-47e...|\n",
      "|e1f07fac-5566-4fd...|        false|Git Fundamentals ...|1516746279.3801291|5a67b627cc80e6000...|1516746279.3801291|         1.0|Map(questions -> ...|2018-01-23T22:24:...|48d88326-36a3-4cb...|\n",
      "|7e2e0b53-a7ba-458...|        false|Introduction to P...| 1516743820.305464|5a67ac8cb0a5f4000...| 1516743820.305464|         1.0|Map(questions -> ...|2018-01-23T21:43:...|bb152d6b-cada-41e...|\n",
      "|1a233da8-e6e5-48a...|        false|Intermediate Pyth...|  1516743098.56811|5a67a9ba060087000...|  1516743098.56811|         1.0|Map(questions -> ...|2018-01-23T21:31:...|70073d6f-ced5-4d0...|\n",
      "|7e2e0b53-a7ba-458...|        false|Introduction to P...| 1516743764.813107|5a67ac54411aed000...| 1516743764.813107|         1.0|Map(questions -> ...|2018-01-23T21:42:...|9eb6d4d6-fd1f-4f3...|\n",
      "|4cdf9b5f-fdb7-4a4...|        false|A Practical Intro...|1516744091.3127241|5a67ad9b2ff312000...|1516744091.3127241|         1.0|Map(questions -> ...|2018-01-23T21:45:...|093f1337-7090-457...|\n",
      "|e1f07fac-5566-4fd...|        false|Git Fundamentals ...|1516746256.5878439|5a67b610baff90000...|1516746256.5878439|         1.0|Map(questions -> ...|2018-01-23T22:24:...|0f576abb-958a-4c0...|\n",
      "|87b4b3f9-3a86-435...|        false|Introduction to M...|  1516743832.99235|5a67ac9837b82b000...|  1516743832.99235|         1.0|Map(questions -> ...|2018-01-23T21:40:...|0c18f48c-0018-450...|\n",
      "|a7a65ec6-77dc-480...|        false|   Python Epiphanies|1516743332.7596769|5a67aaa4f21cc2000...|1516743332.7596769|         1.0|Map(questions -> ...|2018-01-23T21:34:...|b38ac9d8-eef9-495...|\n",
      "|7e2e0b53-a7ba-458...|        false|Introduction to P...| 1516743750.097306|5a67ac46f7bce8000...| 1516743750.097306|         1.0|Map(questions -> ...|2018-01-23T21:41:...|bbc9865f-88ef-42e...|\n",
      "|e5602ceb-6f0d-11e...|        false|Python Data Struc...|1516744410.4791961|5a67aedaf34e85000...|1516744410.4791961|         1.0|Map(questions -> ...|2018-01-23T21:51:...|8a0266df-02d7-44e...|\n",
      "|e5602ceb-6f0d-11e...|        false|Python Data Struc...|1516744446.3999851|5a67aefef5e149000...|1516744446.3999851|         1.0|Map(questions -> ...|2018-01-23T21:53:...|95d4edb1-533f-445...|\n",
      "|f432e2e3-7e3a-4a7...|        false|Working with Algo...| 1516744255.840405|5a67ae3f0c5f48000...| 1516744255.840405|         1.0|Map(questions -> ...|2018-01-23T21:50:...|f9bc1eff-7e54-42a...|\n",
      "|76a682de-6f0c-11e...|        false|Learning iPython ...| 1516744023.652257|5a67ad579d5057000...| 1516744023.652257|         1.0|Map(questions -> ...|2018-01-23T21:46:...|dc4b35a7-399a-4bd...|\n",
      "|a7a65ec6-77dc-480...|        false|   Python Epiphanies|1516743398.6451161|5a67aae6753fd6000...|1516743398.6451161|         1.0|Map(questions -> ...|2018-01-23T21:35:...|d0f8249a-597e-4e1...|\n",
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_assessments = assessments.rdd.map(lambda x: Row(**json.loads(x.value))).toDF()\n",
    "\n",
    "extracted_assessments.registerTempTable('assessments')\n",
    "extracted_assessments.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 and 5) Register Dataframes as Temporary Tables to Allow in Memory Queries Against them AND Perform SQL Queries Against the Dataframes You Registered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_assessments.registerTempTable('assessments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             keen_id|\n",
      "+--------------------+\n",
      "|5a6745820eb8ab000...|\n",
      "|5a674541ab6b0a000...|\n",
      "|5a67999d3ed3e3000...|\n",
      "|5a6799694fc7c7000...|\n",
      "|5a6791e824fccd000...|\n",
      "|5a67a0b6852c2a000...|\n",
      "|5a67b627cc80e6000...|\n",
      "|5a67ac8cb0a5f4000...|\n",
      "|5a67a9ba060087000...|\n",
      "|5a67ac54411aed000...|\n",
      "+--------------------+\n",
      "\n",
      "+------------------+-------------------------------------------------------+\n",
      "|    keen_timestamp|sequences[questions] AS `questions`[0][user_incomplete]|\n",
      "+------------------+-------------------------------------------------------+\n",
      "| 1516717442.735266|                                                   true|\n",
      "| 1516717377.639827|                                                  false|\n",
      "| 1516738973.653394|                                                  false|\n",
      "|1516738921.1137421|                                                  false|\n",
      "| 1516737000.212122|                                                  false|\n",
      "| 1516740790.309757|                                                  false|\n",
      "|1516746279.3801291|                                                  false|\n",
      "| 1516743820.305464|                                                  false|\n",
      "|  1516743098.56811|                                                  false|\n",
      "| 1516743764.813107|                                                  false|\n",
      "+------------------+-------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_assessments = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"kafka:29092\").option(\"subscribe\",\"assessments\").option(\"startingOffsets\", \"earliest\").option(\"endingOffsets\", \"latest\").load() \n",
    "\n",
    "raw_assessments.cache()\n",
    "\n",
    "assessments = raw_assessments.select(raw_assessments.value.cast('string'))\n",
    "\n",
    "import json\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "extracted_assessments = assessments.rdd.map(lambda x: Row(**json.loads(x.value))).toDF()\n",
    "\n",
    "extracted_assessments.registerTempTable('assessments')\n",
    "\n",
    "spark.sql(\"select keen_id from assessments limit 10\").show()\n",
    "\n",
    "spark.sql(\"select keen_timestamp, sequences.questions[0].user_incomplete from assessments limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the exam_names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           exam_name|\n",
      "+--------------------+\n",
      "|Normal Forms and ...|\n",
      "|Normal Forms and ...|\n",
      "|The Principles of...|\n",
      "|The Principles of...|\n",
      "|Introduction to B...|\n",
      "|        Learning Git|\n",
      "|Git Fundamentals ...|\n",
      "|Introduction to P...|\n",
      "|Intermediate Pyth...|\n",
      "|Introduction to P...|\n",
      "|A Practical Intro...|\n",
      "|Git Fundamentals ...|\n",
      "|Introduction to M...|\n",
      "|   Python Epiphanies|\n",
      "|Introduction to P...|\n",
      "|Python Data Struc...|\n",
      "|Python Data Struc...|\n",
      "|Working with Algo...|\n",
      "|Learning iPython ...|\n",
      "|   Python Epiphanies|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select exam_name from assessments\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the highest number of students who took each exam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------+------------------+\n",
      "|exam_name                                                     |number_of_students|\n",
      "+--------------------------------------------------------------+------------------+\n",
      "|Learning Git                                                  |394               |\n",
      "|Introduction to Python                                        |162               |\n",
      "|Introduction to Java 8                                        |158               |\n",
      "|Intermediate Python Programming                               |158               |\n",
      "|Learning to Program with R                                    |128               |\n",
      "|Introduction to Machine Learning                              |119               |\n",
      "|Software Architecture Fundamentals Understanding the Basics   |109               |\n",
      "|Beginning C# Programming                                      |95                |\n",
      "|Learning Eclipse                                              |85                |\n",
      "|Learning Apache Maven                                         |80                |\n",
      "|Beginning Programming with JavaScript                         |79                |\n",
      "|Mastering Git                                                 |77                |\n",
      "|Introduction to Big Data                                      |75                |\n",
      "|Advanced Machine Learning                                     |67                |\n",
      "|Learning Linux System Administration                          |59                |\n",
      "|JavaScript: The Good Parts Master Class with Douglas Crockford|58                |\n",
      "|Learning SQL                                                  |57                |\n",
      "|Practical Java Programming                                    |53                |\n",
      "|HTML5 The Basics                                              |52                |\n",
      "|Python Epiphanies                                             |51                |\n",
      "+--------------------------------------------------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select exam_name, count(*) as number_of_students from assessments group by exam_name order by number_of_students desc\").show(truncate = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the lowest number of students who took each exam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+------------------+\n",
      "|exam_name                                         |number_of_students|\n",
      "+--------------------------------------------------+------------------+\n",
      "|Learning to Visualize Data with D3.js             |1                 |\n",
      "|Native Web Apps for Android                       |1                 |\n",
      "|Nulls, Three-valued Logic and Missing Information |1                 |\n",
      "|Operating Red Hat Enterprise Linux Servers        |1                 |\n",
      "|Learning Spring Programming                       |2                 |\n",
      "|Client-Side Data Storage for Web Developers       |2                 |\n",
      "|Understanding the Grails 3 Domain Model           |2                 |\n",
      "|The Closed World Assumption                       |2                 |\n",
      "|Hibernate and JPA Fundamentals                    |2                 |\n",
      "|What's New in JavaScript                          |2                 |\n",
      "|Arduino Prototyping Techniques                    |2                 |\n",
      "|Building Web Services with Java                   |3                 |\n",
      "|Service Based Architectures                       |3                 |\n",
      "|Getting Ready for Angular 2                       |3                 |\n",
      "|Using Web Components                              |3                 |\n",
      "|Mastering Web Views                               |3                 |\n",
      "|Using Storytelling to Effectively Communicate Data|4                 |\n",
      "|View Updating                                     |4                 |\n",
      "|Starting a Grails 3 Project                       |5                 |\n",
      "|An Introduction to Set Theory                     |5                 |\n",
      "+--------------------------------------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select exam_name, count(*) as number_of_students from assessments group by exam_name order by number_of_students asc\").show(truncate = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def my_lambda_sequences_id(x):\n",
    "    raw_dict = json.loads(x.value)\n",
    "    my_dict = {\"keen_id\" : raw_dict[\"keen_id\"], \"sequences_id\" : raw_dict[\"sequences\"][\"id\"]}\n",
    "    return Row(**my_dict)\n",
    "\n",
    "assessments.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sequences = assessments.rdd.map(my_lambda_sequences_id).toDF()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_sequences.registerTempTable('sequences')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        sequences_id|\n",
      "+--------------------+\n",
      "|5b28a462-7a3b-42e...|\n",
      "|5b28a462-7a3b-42e...|\n",
      "|b370a3aa-bf9e-4c1...|\n",
      "|b370a3aa-bf9e-4c1...|\n",
      "|04a192c1-4f5c-4ac...|\n",
      "|e7110aed-0d08-4cb...|\n",
      "|5251db24-2a6e-424...|\n",
      "|066b5326-e547-4da...|\n",
      "|8ac691f8-8c1a-403...|\n",
      "|066b5326-e547-4da...|\n",
      "+--------------------+\n",
      "\n",
      "+--------------------+------------------+--------------------+\n",
      "|             keen_id|    keen_timestamp|        sequences_id|\n",
      "+--------------------+------------------+--------------------+\n",
      "|5a17a67efa1257000...|1511499390.3836269|8ac691f8-8c1a-403...|\n",
      "|5a26ee9cbf5ce1000...|1512500892.4166169|9bd87823-4508-4e0...|\n",
      "|5a29dcac74b662000...|1512692908.8423469|e7110aed-0d08-4cb...|\n",
      "|5a2fdab0eabeda000...|1513085616.2275269|cd800e92-afc3-447...|\n",
      "|5a30105020e9d4000...|1513099344.8624721|8ac691f8-8c1a-403...|\n",
      "|5a3a6fc3f0a100000...| 1513779139.354213|e7110aed-0d08-4cb...|\n",
      "|5a4e17fe08a892000...|1515067390.1336551|9abd5b51-6bd8-11e...|\n",
      "|5a4f3c69cc6444000...| 1515142249.858722|083844c5-772f-48d...|\n",
      "|5a51b21bd0480b000...| 1515303451.773272|e7110aed-0d08-4cb...|\n",
      "|5a575a85329e1a000...| 1515674245.348099|25ca21fe-4dbb-446...|\n",
      "+--------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select sequences_id from sequences limit 10\").show()\n",
    "\n",
    "spark.sql(\"select a.keen_id, a.keen_timestamp, s.sequences_id from assessments a join sequences s on a.keen_id = s.keen_id limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                  id|my_count|\n",
      "+--------------------+--------+\n",
      "|7a2ed6d3-f492-49b...|       1|\n",
      "|bbed4358-999d-446...|       2|\n",
      "|e6ad8644-96b1-461...|       3|\n",
      "|95194331-ac43-454...|       4|\n",
      "|95194331-ac43-454...|       1|\n",
      "|bbed4358-999d-446...|       2|\n",
      "|e6ad8644-96b1-461...|       3|\n",
      "|7a2ed6d3-f492-49b...|       4|\n",
      "|b9ff2e88-cf9d-4bd...|       1|\n",
      "|bec23e7b-4870-49f...|       2|\n",
      "+--------------------+--------+\n",
      "\n",
      "+--------------------+------------------+--------------------+\n",
      "|             keen_id|    keen_timestamp|                  id|\n",
      "+--------------------+------------------+--------------------+\n",
      "|5a17a67efa1257000...|1511499390.3836269|803fc93f-7eb2-412...|\n",
      "|5a17a67efa1257000...|1511499390.3836269|f3cb88cc-5b79-41b...|\n",
      "|5a17a67efa1257000...|1511499390.3836269|32fe7d8d-6d89-4db...|\n",
      "|5a17a67efa1257000...|1511499390.3836269|5c34cf19-8cfd-4f5...|\n",
      "|5a26ee9cbf5ce1000...|1512500892.4166169|0603e6f4-c3f9-4c2...|\n",
      "|5a26ee9cbf5ce1000...|1512500892.4166169|26a06b88-2758-45b...|\n",
      "|5a26ee9cbf5ce1000...|1512500892.4166169|25b6effe-79b0-4c4...|\n",
      "|5a26ee9cbf5ce1000...|1512500892.4166169|6de03a9b-2a78-46b...|\n",
      "|5a26ee9cbf5ce1000...|1512500892.4166169|aaf39991-fa83-470...|\n",
      "|5a26ee9cbf5ce1000...|1512500892.4166169|aab2e817-73dc-4ff...|\n",
      "+--------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def my_lambda_questions(x):\n",
    "    raw_dict = json.loads(x.value)\n",
    "    my_list = []\n",
    "    my_count = 0\n",
    "    for l in raw_dict[\"sequences\"][\"questions\"]:\n",
    "        my_count += 1\n",
    "        my_dict = {\"keen_id\" : raw_dict[\"keen_id\"], \"my_count\" : my_count, \"id\" : l[\"id\"]}\n",
    "        my_list.append(Row(**my_dict))\n",
    "    return my_list\n",
    "\n",
    "my_questions = assessments.rdd.flatMap(my_lambda_questions).toDF()\n",
    "\n",
    "my_questions.registerTempTable('questions')\n",
    "\n",
    "spark.sql(\"select id, my_count from questions limit 10\").show()\n",
    "\n",
    "spark.sql(\"select q.keen_id, a.keen_timestamp, q.id from assessments a join questions q on a.keen_id = q.keen_id limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which keen.id has most and least questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keen.id's with the most questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                  id|my_count|\n",
      "+--------------------+--------+\n",
      "|7bdbbf4a-b5d8-4c7...|       1|\n",
      "|dccd58b0-251b-40f...|       1|\n",
      "|d2ac7f0d-82bd-415...|       1|\n",
      "|95194331-ac43-454...|       1|\n",
      "|59d444b5-49fd-487...|       1|\n",
      "|1f7c5def-904b-483...|       1|\n",
      "|e272a3d1-bd67-4d2...|       1|\n",
      "|fb07b16e-84a2-465...|       1|\n",
      "|dee14932-a24e-4aa...|       1|\n",
      "|fc3bdc54-04a8-4b4...|       1|\n",
      "|861c3405-83fc-42f...|       1|\n",
      "|fc3bdc54-04a8-4b4...|       1|\n",
      "|f289b342-2871-4ab...|       1|\n",
      "|247b4589-7f8c-4a4...|       1|\n",
      "|aa7a63cd-7aa4-47f...|       1|\n",
      "|26ddad33-aa1d-49e...|       1|\n",
      "|e272a3d1-bd67-4d2...|       1|\n",
      "|917b9413-f83b-4f2...|       1|\n",
      "|c3e6e626-884a-4be...|       1|\n",
      "|34eacafb-dd6a-424...|       1|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select id, my_count from questions order by my_count\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keen.id's with the least questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                  id|my_count|\n",
      "+--------------------+--------+\n",
      "|8d5372d4-a63b-40c...|      20|\n",
      "|7d527603-ed07-4d1...|      19|\n",
      "|bdc4d043-b161-426...|      18|\n",
      "|2494e12b-070e-458...|      17|\n",
      "|a994e9ca-208a-40a...|      16|\n",
      "|ebc3d26e-ed70-4cd...|      15|\n",
      "|ddac0ade-2320-48d...|      14|\n",
      "|0bc8696a-b9b0-43b...|      13|\n",
      "|c27494a4-fe24-4a1...|      12|\n",
      "|be969a50-0474-409...|      11|\n",
      "|7193e678-1989-4bf...|      10|\n",
      "|283dee13-3e46-480...|      10|\n",
      "|e3cc7e9c-603a-48c...|      10|\n",
      "|bc7ae396-a747-4f8...|      10|\n",
      "|48f0cecb-80f7-4b4...|      10|\n",
      "|7193e678-1989-4bf...|      10|\n",
      "|7193e678-1989-4bf...|      10|\n",
      "|283dee13-3e46-480...|      10|\n",
      "|48f0cecb-80f7-4b4...|      10|\n",
      "|bc7ae396-a747-4f8...|      10|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select id, my_count from questions order by my_count desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|correct|total|\n",
      "+-------+-----+\n",
      "|      2|    4|\n",
      "|      1|    4|\n",
      "|      3|    4|\n",
      "|      2|    4|\n",
      "|      3|    4|\n",
      "|      5|    5|\n",
      "|      1|    1|\n",
      "|      5|    5|\n",
      "|      4|    4|\n",
      "|      0|    5|\n",
      "+-------+-----+\n",
      "\n",
      "+-----+\n",
      "|score|\n",
      "+-----+\n",
      "|  0.5|\n",
      "| 0.25|\n",
      "| 0.75|\n",
      "|  0.5|\n",
      "| 0.75|\n",
      "|  1.0|\n",
      "|  1.0|\n",
      "|  1.0|\n",
      "|  1.0|\n",
      "|  0.0|\n",
      "+-----+\n",
      "\n",
      "+-----------------+\n",
      "|        avg_score|\n",
      "+-----------------+\n",
      "|62.65699745547047|\n",
      "+-----------------+\n",
      "\n",
      "+-------------------+\n",
      "| standard_deviation|\n",
      "+-------------------+\n",
      "|0.31086692286170553|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def my_lambda_correct_total(x):\n",
    "    \n",
    "    raw_dict = json.loads(x.value)\n",
    "    my_list = []\n",
    "    \n",
    "    if \"sequences\" in raw_dict:\n",
    "        \n",
    "        if \"counts\" in raw_dict[\"sequences\"]:\n",
    "            \n",
    "            if \"correct\" in raw_dict[\"sequences\"][\"counts\"] and \"total\" in raw_dict[\"sequences\"][\"counts\"]:\n",
    "                    \n",
    "                my_dict = {\"correct\": raw_dict[\"sequences\"][\"counts\"][\"correct\"], \n",
    "                           \"total\": raw_dict[\"sequences\"][\"counts\"][\"total\"]}\n",
    "                my_list.append(Row(**my_dict))\n",
    "    \n",
    "    return my_list\n",
    "\n",
    "my_correct_total = assessments.rdd.flatMap(my_lambda_correct_total).toDF()\n",
    "\n",
    "my_correct_total.registerTempTable('ct')\n",
    "\n",
    "spark.sql(\"select * from ct limit 10\").show()\n",
    "\n",
    "spark.sql(\"select correct / total as score from ct limit 10\").show()\n",
    "\n",
    "spark.sql(\"select avg(correct / total)*100 as avg_score from ct limit 10\").show()\n",
    "\n",
    "spark.sql(\"select stddev(correct / total) as standard_deviation from ct limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 perform a write to HDFS in parquet format for each data frame you created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_assessments.write.mode('overwrite').parquet(\"/tmp/raw_assessments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_sequences.write.mode('overwrite').parquet(\"/tmp/sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_questions.write.mode('overwrite').parquet(\"/tmp/questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_correct_total.write.mode('overwrite').parquet(\"/tmp/correct_total\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
